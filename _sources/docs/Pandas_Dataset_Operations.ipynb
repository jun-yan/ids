{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c0e3e8",
   "metadata": {},
   "source": [
    "# Pandas: Dataset Operations\n",
    "\n",
    "Dataset can be combined in a multidue of different ways from other datasets. Operations that can be used can be from a straightforward approch such as using concatenation from two datasets. Alternatively, one could use database-style joins and merges to correctly handle any overlaps between datasets. \n",
    "\n",
    "`Series` and `DataFrames` both use the concatenation function, and Pandas includes functions and methods that make this sort of data wrangling fast and straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f353f7",
   "metadata": {},
   "source": [
    "## Simple Concatenation Using pd.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df477e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pd. __version__\n",
    "\n",
    "def create_df(cols, ind):\n",
    "    data = {c: [str(c) + str(i) for i in ind] for c in cols}\n",
    "    return pd.DataFrame(data, ind)\n",
    "\n",
    "create_df(['red', 'green', 'blue'], range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3dbb0b",
   "metadata": {},
   "source": [
    "`pd.concat()` can be used for simple concatenation of Series and DataFrames, it defaults to row-wise but can be specified to take place along any axis. \n",
    "  \n",
    "Here is an example of row-wise concatination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b7cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "series1 = pd.Series([\"red\", \"green\", \"blue\"], [1, 2, 3])\n",
    "series2 = pd.Series([\"yellow\", \"orange\", \"purple\"], [4, 5, 6])\n",
    "series_concat = pd.concat([series1, series2])\n",
    "print(series1); print(); \n",
    "print(series2); print(); \n",
    "print(series_concat); print() # final result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a0e754",
   "metadata": {},
   "source": [
    "We can concatinate two DataFrames that have shared columns and merge the columns together, as seen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562430d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = create_df([\"red\", \"green\"], [0, 1])\n",
    "df2 = create_df([\"red\", \"green\"], [2, 3])\n",
    "df_concat1 = pd.concat([df1, df2])\n",
    "print(df1); print();\n",
    "print(df2); print();\n",
    "print(df_concat1); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6eb20",
   "metadata": {},
   "source": [
    "We can also concatinate column-wise by passing the argument `axis = 1` or `axis = 'col'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = create_df([\"red\", \"green\"], [0, 1])\n",
    "df4 = create_df([\"blue\", \"yellow\"], [0, 1])\n",
    "df_concat2 = pd.concat([df3, df4], axis = 1) # axis = 'col'\n",
    "print(df3); print();\n",
    "print(df4); print();\n",
    "print(df_concat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d2238",
   "metadata": {},
   "source": [
    "### Concatinating with Duplicate Indices\n",
    "`pd.concat()` can be used to handle situations where you concatinate with dulpicate indicies. By default it raises an error if this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = create_df([\"red\", \"green\"], [0, 1])\n",
    "y = create_df([\"red\", \"green\"], [2, 3])\n",
    "y.index = x.index\n",
    "\n",
    "try:\n",
    "    pd.concat([x, y], verify_integrity = True) # duplicated indices\n",
    "except ValueError as e:\n",
    "    print(\"ValueError:\", e); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501cc24",
   "metadata": {},
   "source": [
    "You can bypass this error by passing the argument `ignore_index = True` which will cause `concat` to ignore the indicies of the DataFrames it is concatinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6866c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat3 = pd.concat([x, y], ignore_index = True)\n",
    "print(x); print();\n",
    "print(y); print();\n",
    "print(df_concat3); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a510b",
   "metadata": {},
   "source": [
    "Another option is to pass an argument specifying keys. The `keys` argument must be a list, tuple, or some other sequence which corisponds to the DataFrames you are concatenating. Here we use `keys = [\"x\", \"y\"]` to add x and y as keys for the two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat4 = pd.concat([x, y], keys = [\"x\", \"y\"])\n",
    "print(x); print();\n",
    "print(y); print();\n",
    "print(df_concat4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080d1f5",
   "metadata": {},
   "source": [
    "### Concatenations with joins\n",
    "\n",
    "When concatinating dataframes with different column names `pd.concat` defaults to filling entries where no data is available with NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = create_df([\"red\", \"green\", \"cyan\"], [1, 2, 3])\n",
    "df6 = create_df([\"green\", \"cyan\", \"blue\"], [4, 5, 6])\n",
    "print(df5); print()\n",
    "print(df6); print()\n",
    "print(pd.concat([df5, df6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91acb89d",
   "metadata": {},
   "source": [
    "In order to remove the NA's we want to use the argument `join` to concatenate the function. The default statement is a union join, `join = 'outer'`. This can be changed to an intersection of the columns by using `join = 'inner'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9388bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df5); print();\n",
    "print(df6); print();\n",
    "print(pd.concat([df5, df6], join = 'inner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e752cf5",
   "metadata": {},
   "source": [
    "### Append() Method\n",
    "\n",
    "Direct array concatentation is very common, `series` and `DataFrame` objects have an additional method that can be used in similar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d95111",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1); print();\n",
    "print(df2); print();\n",
    "print(df1.append(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91f83d",
   "metadata": {},
   "source": [
    "`.append()` is a very simple function that does not check for duplicate indicies and will create a new DataFrame that is simply both DataFrames together, as seen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc322f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1); print();\n",
    "print(df1); print();\n",
    "print(df1.append(df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6726d",
   "metadata": {},
   "source": [
    "## Combining Datasets: Merge and Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818dc086",
   "metadata": {},
   "source": [
    "### Categories of Joins\n",
    "\n",
    "Using the `pd.merge()` function carries out a number of types of joins: *one-to-one*, *many-to-one*, and *many-to-many* joins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb9267",
   "metadata": {},
   "source": [
    "#### One-to-One Join\n",
    "A one-to-one join is perhaps the simplist, being very simmilar to column-wise concatenation. To do this we simply run `pd.merge()` with our data frames as arguments. In this case `pd.merge()` sees that the `ID` column is shared between df7 and df8 and will use it as a key to merge the two. What results is an intersection of the two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.DataFrame({\n",
    "    'ID': [101, 102, 103, 104, 105, 106, 107],\n",
    "    'Product_Name': ['SmartWatch', 'Backpack', 'Shoes', 'Smartphone',\n",
    "                     'Books', 'Oil','Laptop'],\n",
    "    'Category': ['Electronics', 'Study', 'Fashion', 'Electronics',\n",
    "                 'Study', 'Grocery', 'Electronics'],\n",
    "    'Price': [299.0, 150.50, 2999.0, 14999.0, 145.0, 110.0, 79999.0]\n",
    "})\n",
    "\n",
    "df8 = pd.DataFrame({\n",
    "    'Ref_Num': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'Name': ['Ollie', 'Ivy', 'Ethan', 'Maya', \n",
    "             'Lucas', 'Levi', 'Miles', 'Daniel', \n",
    "             'Owen'],\n",
    "    'Age': [20, 25, 15, 10, 30, 65, 35, 18, 23],\n",
    "    'ID': [101, 0, 106, 0, 103, 104, 0, 0, 107],\n",
    "    'Purchased_Product': ['SmartWatch', 'NA', 'Oil', 'NA', 'Shoes', \n",
    "                          'Smartphone','NA','NA','Laptop']\n",
    "})\n",
    "\n",
    "df7 # Left DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbf6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 # Right DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f86ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pd.merge()\n",
    "df9 = pd.merge(df7, df8)\n",
    "df9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b90203",
   "metadata": {},
   "source": [
    "#### Many-to-One Joins\n",
    "\n",
    "Many-to-one joins are joins in which one of the two key columns contains duplicated entries. This is simmilarly done by calling `pd.merge()` and will result in the new DataFrame preserving the duplicated entries as appropriate. In this example the `Category` columns are shared and used as a key to merge the two DataFrames. In essence a new column was added to df9 called `Need` using df10 almost like a dictionary to determine what goes in `Need` for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.DataFrame({\n",
    "    'Category': ['Electronics', 'Fashion', 'Grocery'],\n",
    "    'Need': ['Personal', 'Unnecessary', 'Necessary']\n",
    "})\n",
    "\n",
    "df9 # Left Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 # Right DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pd.merge()\n",
    "pd.merge(df9, df10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566f18a",
   "metadata": {},
   "source": [
    "#### Many-to-Many Joins\n",
    "\n",
    "Many-to-many joins if the key column in both left and right DataFrame contains any duplicates, it can result in many-to-many joins. Calling `pd.merge()` will preserve the duplicate entries for both. In this example the key column is `Category` and for both DataFrames there are some duplicate entries. It preserves these by creating new instances for each combination possible. So for here there are 3 entries for Electronics for df7 and 2 entries for Electronics for df11. Combining those there is now 6 entries for Electronics, representing the 6 possible combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab16d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = pd.DataFrame({\n",
    "    'Category': ['Electronics', 'Electronics', 'Study', 'Study',\n",
    "                 'Fashion', 'Fashion', 'Grocery', 'Grocery'],\n",
    "    'Occupation': ['College', 'Part-Time', 'Retired', 'Not Working',\n",
    "                   'Working', 'Full-Time', 'High School', 'Chores']\n",
    "})\n",
    "\n",
    "df7 # Left DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af69902",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 # Right DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pd.merge()\n",
    "pd.merge(df7, df11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e56244",
   "metadata": {},
   "source": [
    "### Using Merge Keys\n",
    "\n",
    "By default, `pd.merge()` looks for one or more matching column names to use as a key. However you can specify this column name using the argument `on = 'column_name'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5dd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df7); print()\n",
    "print(df11); print()\n",
    "\n",
    "# using on \n",
    "print(pd.merge(df7, df11, on = 'Category')); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224061ea",
   "metadata": {},
   "source": [
    "If two colums that are the same have different names you can merge them using `left_on` and `right_on` to specify which column for each DataFrame is supposed to be the key. This will also preserve both columns in the resulting DataFrame, however they will be identical besides the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10067fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = pd.DataFrame({\n",
    "    'Type': ['Electronics', 'Electronics', 'Study', 'Study',\n",
    "                 'Fashion', 'Fashion', 'Grocery', 'Grocery'],\n",
    "    'Occupation': ['College', 'Part-Time', 'Retired', 'Not Working',\n",
    "                   'Working', 'Full-Time', 'High School', 'Chores']\n",
    "})\n",
    "\n",
    "print(df7); print()\n",
    "print(df12); print()\n",
    "\n",
    "# using left_on and right_on\n",
    "print(pd.merge(df7, df12, left_on = 'Category', right_on = 'Type'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0921590",
   "metadata": {},
   "source": [
    "It is even possible to merge by index using the arguments `left_index = True` and `right_index = True` to specify you want to merge by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f641e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df13 = pd.DataFrame({\n",
    "    'Name': ['Kelly', 'Ann', 'Suzy', 'Bob',\n",
    "            'Sydney', 'Jacob', 'Will'],\n",
    "    'Occupation': ['College', 'Part-Time', 'Retired', 'Not Working',\n",
    "                   'Working', 'Full-Time', 'High School']\n",
    "})\n",
    "\n",
    "df14 = pd.DataFrame({\n",
    "    'Name': ['Jacob', 'Sydney', 'Suzy', 'Will',\n",
    "             'Ann', 'Kelly', 'Bob'],\n",
    "    'Favorite_Color': ['Blue', 'Red', 'Green', 'Yellow',\n",
    "                       'Orange', 'Brown', 'Purple']\n",
    "})\n",
    "\n",
    "df13 = df13.set_index('Name')\n",
    "df14 = df14.set_index('Name')\n",
    "print(df13); print()\n",
    "print(df14); print()\n",
    "\n",
    "# Using left_index and right_index\n",
    "print(pd.merge(df13, df14, left_index = True, right_index = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877aa395",
   "metadata": {},
   "source": [
    "## Aggregation and Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a9e53",
   "metadata": {},
   "source": [
    "### Simple Aggregration \n",
    "\n",
    "For Pandas `DataFrame` aggrigates return results within each column. All common aggrigates are available, and in addition there is a method `describe()` which computes several common aggrigates at once for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec463209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install seaborn\n",
    "import seaborn as sns\n",
    "mpg = sns.load_dataset('mpg')\n",
    "\n",
    "mpg.dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfe64da",
   "metadata": {},
   "source": [
    "### GroupBy\n",
    "\n",
    "Conditional Aggrigation by some label or index can be done by `groupby` operation, which does the \"split, apply, combine\" operation by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81716cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.dropna().groupby('origin').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6eb8b9",
   "metadata": {},
   "source": [
    "### Using GroupBy Object\n",
    "\n",
    "It is possible to think of the `GroupBy` object as a collection of `DataFrames`, and it has a variety of operations that can be used. It is possible to index a `GroupBy` object as you would a `DataFrame` to return a modified GroupBy object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a602fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the mpg data grouped by origin to look at the median\n",
    "mpg.groupby('origin')['mpg'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec47a93",
   "metadata": {},
   "source": [
    "The `GroupBy` object also supports direct iteration over groups, returning each group as a `Series` or `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the origin \n",
    "for (origin, group) in mpg.groupby('origin'):\n",
    "    print(\"{0:30s} shape = {1}\".format(origin, group.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10579628",
   "metadata": {},
   "source": [
    "In addition any method not specifically called by the `GroupBy` object will be called on the indivdual groups within the `GroupBy` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the describe() method to each group after\n",
    "# grouping by region of origin\n",
    "mpg.groupby('origin')['mpg'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a67a3",
   "metadata": {},
   "source": [
    "### GroupBy Aggregration \n",
    "\n",
    "The aggregate function can take a string or function or list of those and compute all aggregates at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df17 = pd.DataFrame({\n",
    "    'Color': ['Red', 'Green', 'Blue', 'Red', 'Green', \"Blue\"],\n",
    "    'Data1': range(6),\n",
    "    'Data2': np.random.randint(0, 10, 6)},\n",
    "    columns = ['Color', 'Data1', 'Data2']\n",
    ")\n",
    "\n",
    "# Returns the groups by color looking at the aggregates \n",
    "df17.groupby('Color').aggregate(['min', np.median, max])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9194bc",
   "metadata": {},
   "source": [
    "You can also pass a dictionary which maps colum names to operations to be used in those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e64a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns only the min for data1 and max for data2\n",
    "df17.groupby('Color').aggregate({'Data1':'min',\n",
    "                                 'Data2':'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db53c4a3",
   "metadata": {},
   "source": [
    "### GroupBy Filtering \n",
    "\n",
    "Filtering allows us to keep data based on group properties. It returns a Boolian value saying if the group passes the filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep data only if standard deviation is greater than 3\n",
    "def filter_function(x):\n",
    "    return x['Data2'].std() > 4\n",
    "\n",
    "print(df17); print()\n",
    "print(df17.groupby('Color').std()); print()\n",
    "print(df17.groupby('Color').filter(filter_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21d77b",
   "metadata": {},
   "source": [
    "### GroupBy apply() Method\n",
    "\n",
    "`apply()` lets you apply an arbitrary function to the results of a group. The function takes a `DataFrame` as an argument and returns either a Pandas `DataFrame`, Pandas `Series` or a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divides data1 by the mean of data2\n",
    "def dev_by_mean_data2(x):\n",
    "    x['Data1'] /= x['Data2'].mean()\n",
    "    return(x)\n",
    "\n",
    "print(df17); print()\n",
    "print(df17.groupby('Color').apply(dev_by_mean_data2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f76e9",
   "metadata": {},
   "source": [
    "### Specifying the Split Key\n",
    "\n",
    "The `DataFrame` can be split by more than just a single column name. It can be split by any list, array, series, or index providing the grouping keys so long as the length matches the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67657537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split using a list of groups in order for each \n",
    "# index in the dataframe\n",
    "L = [1, 0, 1, 2, 3, 1]\n",
    "print(df17); print()\n",
    "print(df17.groupby(L).sum()); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825be468",
   "metadata": {},
   "source": [
    "It can also be split by a dictionary which maps index values to group keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split using a dictionary mapping colors to if \n",
    "# they are primary or secondary\n",
    "df18 = df17.set_index('Color')\n",
    "mapping = {'Red':'Primary', 'Blue':'Primary', 'Green':'Secondary'}\n",
    "print(df18); print()\n",
    "print(df18.groupby(mapping).sum()); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32adf5d",
   "metadata": {},
   "source": [
    "It can also be split by any Python function so long as it inputs the index value and outputs the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split using str.upper to make all indicies uppercase\n",
    "print(df18); print()\n",
    "print(df18.groupby(str.upper).mean()); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6bc723",
   "metadata": {},
   "source": [
    "Finally it can be done by mixing any of these together in a list of valid keys to create a multi-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split using both str.upper and the dictionary mapping \n",
    "# colors to primary and secondary\n",
    "print(df18); print()\n",
    "print(df18.groupby([str.upper, mapping]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b29f5d",
   "metadata": {},
   "source": [
    "## Pivot Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617a336",
   "metadata": {},
   "source": [
    "### Basics\n",
    "\n",
    "While `groupby` is useful for gaining basic understanding of data, it can become messy when you try to do anything in more than one-dimension. This is why Pandas has the built in routine `pivot_table` which can easily handle multidimensional aggregation. `pivot_table` allows us to generate a new table of aggrigates which can be broken down further to allow for multidimensional analysis. For example here we are finding the mean mpg for cars based on region of origin and their number of cylinders at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.pivot_table('mpg', index = 'origin', columns = 'cylinders')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8146f",
   "metadata": {},
   "source": [
    "### Multilevel Pivot Tables\n",
    "\n",
    "We can bin data to show multilevel tables using the `pd.cut` and `pd.qcut` functions. Here are examples of three-dimensional and four-dimensional tables done in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7baa83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-dimensional table looking at model year as a third dimension\n",
    "years = pd.cut(mpg['model_year'], [70, 73, 76, 79, 82])\n",
    "mpg.pivot_table('mpg', ['origin', years], 'cylinders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four-dimensional table dividing the results into two weight categories\n",
    "weight = pd.qcut(mpg['weight'], 2)\n",
    "mpg.pivot_table('mpg', ['origin', years], ['cylinders', weight])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f5778",
   "metadata": {},
   "source": [
    "### Additional Pivot Tables Options\n",
    "\n",
    "There are five arguments we havent covered for pivot tables. `fill_value` and `dropna` deal with missing data. `aggfunc` keyword determines which type of aggrigation is applied, which is mean by default. It can specify `'sum'`, `'mean'`, `'count'`, `'min'`, `'max'`, etc.. or a function for an aggrigation (`np.sum()`, `min()`, `sum()`, etc) It can also be a dictionary mapping a collumn to any of the previous options. When using aggfunc the values keyword is determined automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table looking at both the median mpg and max \n",
    "# horsepower for each cylinder count for each region\n",
    "mpg.pivot_table(index = 'origin', columns = 'cylinders', \n",
    "                aggfunc = {'mpg':'median', 'horsepower':'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53335062",
   "metadata": {},
   "source": [
    "The `margins` keyword can be used to compute totals along each grouping. The name defaults to \"All\" but can be specified using `margins_names = \"your_name\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot tqble looking at mean mpg per region and in all regions\n",
    "mpg.pivot_table('mpg', index = 'origin', columns = 'cylinders',\n",
    "                margins = True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.12.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   13,
   21,
   25,
   38,
   44,
   51,
   55,
   62,
   66,
   73,
   78,
   87,
   91,
   96,
   100,
   105,
   111,
   117,
   121,
   125,
   131,
   135,
   139,
   143,
   147,
   153,
   158,
   182,
   186,
   190,
   196,
   205,
   209,
   212,
   218,
   229,
   233,
   236,
   242,
   248,
   252,
   265,
   269,
   291,
   295,
   301,
   307,
   313,
   315,
   321,
   324,
   328,
   332,
   336,
   340,
   346,
   356,
   360,
   364,
   370,
   378,
   384,
   392,
   398,
   404,
   408,
   415,
   419,
   423,
   427,
   432,
   436,
   442,
   444,
   450,
   456,
   460,
   466,
   471,
   475
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}