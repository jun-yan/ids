{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c3b912",
   "metadata": {},
   "source": [
    "# Fraud Detection with Python (Github Trenton McKinney)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44ca54",
   "metadata": {},
   "source": [
    "The notes are based off of McKinney, T. (2019, July 19). Fraud Detection with Python. Fraud Detection with Python - GitHub (Using Jupyter Book): <https://trenton3983.github.io/files/projects/2019-07-19_fraud_detection_python/2019-07-19_fraud_detection_python.html>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a7856",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fd971",
   "metadata": {},
   "source": [
    "Fraud is intentional deception with the aim of providing the perpetrator with some gain or to deny the rights of a victim. There are a few models to detect fraud in datasets that we will be going over here.Fraud is intentional deception with the aim of providing the perpetrator with some gain or to deny the rights of a victim. There are a few models to detect fraud in datasets that we will be going over here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b92951a",
   "metadata": {},
   "source": [
    "## Setting Up Working Enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec4aa5",
   "metadata": {},
   "source": [
    "First we will be importing what will be used in these notes. The packages imblearn and gensim are not by default installed with anaconda and will need to be installed with pip install `imblearn`, `gensim`, and `nltk`. We will also set our configuration options for Pandas at this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c598a49",
   "metadata": {},
   "source": [
    "### Installation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c244db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8821d5d",
   "metadata": {},
   "source": [
    "Installing necessary pacakges using `!pip3 install` using Jupyter Notebook. Commenting out the installation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install matplotlib\n",
    "#!pip3 install gensim\n",
    "#!pip3 install imblearn\n",
    "#!pip3 install nltk\n",
    "#!pip3 install pyldavis\n",
    "#!pip3 install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d475d62",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b9e2f3",
   "metadata": {},
   "source": [
    "Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ffd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.pipeline import Pipeline \n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import r2_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import homogeneity_score, silhouette_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import MiniBatchKMeans, DBSCAN\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "import wget # helps with installation \n",
    "import zipfile\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa826c1",
   "metadata": {},
   "source": [
    "### Pandas Configuration Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6208ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 700)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.min_rows', 10)\n",
    "pd.set_option('display.expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029ca7b",
   "metadata": {},
   "source": [
    "### Data File Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed2a98",
   "metadata": {},
   "source": [
    "We are going to need some data to check for fraud. This chunk of code will download and unpack the data we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1dbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"chapter_1.zip\"):\n",
    "    wget.download(\"https://assets.datacamp.com/production/repositories/2162/datasets/cc3a36b722c0806e4a7df2634e345975a0724958/chapter_1.zip\")\n",
    "\n",
    "if not os.path.exists(\"chapter_2.zip\"):\n",
    "    wget.download(\"https://assets.datacamp.com/production/repositories/2162/datasets/4fb6199be9b89626dcd6b36c235cbf60cf4c1631/chapter_2.zip\")\n",
    "\n",
    "if not os.path.exists(\"chapter_3.zip\"):\n",
    "    wget.download(\"https://assets.datacamp.com/production/repositories/2162/datasets/08cfcd4158b3a758e72e9bd077a9e44fec9f773b/chapter_3.zip\")\n",
    "\n",
    "if not os.path.exists(\"chapter_4.zip\"):    \n",
    "    wget.download(\"https://assets.datacamp.com/production/repositories/2162/datasets/94f2356652dc9ea8f0654b5e9c29645115b6e77f/chapter_4.zip\")\n",
    "\n",
    "if not os.path.exists(\"data/chapter_1\"): \n",
    "    with zipfile.ZipFile(\"chapter_1.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data\")\n",
    "    os.remove(\"chapter_1.zip\")\n",
    "\n",
    "if not os.path.exists(\"data/chapter_2\"):         \n",
    "    with zipfile.ZipFile(\"chapter_2.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data\")\n",
    "    os.remove(\"chapter_2.zip\")\n",
    "        \n",
    "if not os.path.exists(\"data/chapter_3\"):         \n",
    "    with zipfile.ZipFile(\"chapter_3.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data\")\n",
    "    os.remove(\"chapter_3.zip\")\n",
    "        \n",
    "if not os.path.exists(\"data/chapter_4\"):         \n",
    "    with zipfile.ZipFile(\"chapter_4.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data\")\n",
    "    os.remove(\"chapter_4.zip\")\n",
    "        \n",
    "#!unzip chapter_1.zip\n",
    "#!unzip chapter_2.zip\n",
    "#!unzip chapter_3.zip\n",
    "#!unzip chapter_4.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path.cwd() / 'data'  \n",
    "\n",
    "\n",
    "ch1 = data / 'chapter_1'\n",
    "cc1_file = ch1 / 'creditcard_sampledata.csv'\n",
    "cc3_file = ch1 / 'creditcard_sampledata_3.csv'\n",
    "\n",
    "ch2 = data / 'chapter_2'\n",
    "cc2_file = ch2 / 'creditcard_sampledata_2.csv'\n",
    "\n",
    "ch3 = data / 'chapter_3'\n",
    "banksim_file = ch3 / 'banksim.csv'\n",
    "banksim_adj_file = ch3 / 'banksim_adj.csv'\n",
    "db_full_file = ch3 / 'db_full.pickle'\n",
    "labels_file = ch3 / 'labels.pickle'\n",
    "labels_full_file = ch3 / 'labels_full.pickle'\n",
    "x_scaled_file = ch3 / 'x_scaled.pickle'\n",
    "x_scaled_full_file = ch3 / 'x_scaled_full.pickle'\n",
    "\n",
    "ch4 = data / 'chapter_4'\n",
    "enron_emails_clean_file = ch4 / 'enron_emails_clean.csv'\n",
    "cleantext_file = ch4 / 'cleantext.pickle'\n",
    "corpus_file = ch4 / 'corpus.pickle'\n",
    "dict_file = ch4 / 'dict.pickle'\n",
    "ldamodel_file = ch4 / 'ldamodel.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590f134",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "Fraud occurs only in an extreme minority of transactions. However, machine learning algorithms learn best when the cases they are looking at are fairly even. Without many datapoints of actual fraud, it is difficult to teach it how to detect fraud. This is called a _class imbalance_.  \n",
    "  \n",
    "Lets start by taking the `'creditcard_sampledata.csv'` file and converting it into a Pandas `DataFrame` and looking at the the DataFrame so we know what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8170f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cc3_file) # Create Pandas DataFrame from our file\n",
    "\n",
    "df.info() # Gives information on the type of data in the DataFrame\n",
    "\n",
    "df.head() # Shows the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbebd1",
   "metadata": {},
   "source": [
    "### Visualizing the Fraudulent data\n",
    "\n",
    "Here, the fraudulent status of the data is already known in the `Class` column (0 is non-fraudulent, 1 is fraudulent). The data in columns V1-V28 are about the transactions for each account. Lets take a look at how many fraudulent cases there are and what the ratio of fraudulent to non-fraudulent cases is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cefeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of fraud and no fraud occurances\n",
    "occ = df['Class'].value_counts()\n",
    "occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52bdc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the ratio of fraud to non-fraud cases\n",
    "ratio_cases = occ/len(df.index)\n",
    "print(f'Ratio of fraudulent cases: {ratio_cases[1]}\\nRatio of non-fraudulent cases: {ratio_cases[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ba302",
   "metadata": {},
   "source": [
    "See how low the ratio of fraudulent cases is! This is what we need to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4cfe36",
   "metadata": {},
   "source": [
    "Let's now plot the fraudulent and non-fraudulent data so we get a better idea of what we are working with. Lets define two functions: prep_data() and plot_data(). The first will take in our original DataFrame and return X and y DataFrames only containing the transaction data and if the account is fraudulent respectively. The second will take our data from prep_data() and plot the V2 vs V3 values coloring them by if they are fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfabc79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df: pd.DataFrame) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Convert the DataFrame into two variable\n",
    "    X: data columns (V2 - Amount)\n",
    "    y: lable column\n",
    "    \"\"\"\n",
    "    X = df.iloc[:, 2:30].values\n",
    "    y = df.Class.values\n",
    "    return X, y\n",
    "\n",
    "# Define a function to create a scatter plot of our data and labels with x being V2 and y being V3\n",
    "def plot_data(X: np.ndarray, y: np.ndarray):\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
    "                label = \"Class #0\", alpha = 0.5, linewidth = 0.15)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
    "                label = \"Class #1\", alpha = 0.5, linewidth = 0.15, c = 'r')\n",
    "    plt.xlabel(\"V2\")\n",
    "    plt.ylabel(\"V3\")\n",
    "    plt.legend()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8106d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y from the prep_data function \n",
    "X, y = prep_data(df)\n",
    "\n",
    "# Plot our data by running our plot data function on X and y\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51876aec",
   "metadata": {},
   "source": [
    "### Data Resampling\n",
    "We can resample our data to better account for the imbalance in the dataset. This can be done by Undersampling or Oversampling. To be able to compare the resampled datasets let us define a `compare_plot()` function which will take in two DataFrames and return a comparison of the two plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c03c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot(X: np.ndarray, y: np.ndarray, X_resampled: np.ndarray, y_resampled: np.ndarray, method: str):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
    "                label = \"Class #0\", alpha = 0.5, linewidth = 0.15)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1],\n",
    "                label = \"Class #1\", alpha = 0.5, linewidth = 0.15, c = 'r')\n",
    "    plt.xlabel(\"V2\")\n",
    "    plt.ylabel(\"V3\")\n",
    "    plt.title('Original Set')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X_resampled[y_resampled == 0, 0], \n",
    "                X_resampled[y_resampled == 0, 1], \n",
    "                label = \"Class #0\", alpha = 0.5, linewidth = 0.15)\n",
    "    plt.scatter(X_resampled[y_resampled == 1, 0], \n",
    "                X_resampled[y_resampled == 1, 1], \n",
    "                label = \"Class #1\", alpha = 0.5, linewidth = 0.15, c = 'r')\n",
    "    plt.xlabel(\"V2\")\n",
    "    plt.title(method)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e70da",
   "metadata": {},
   "source": [
    "#### Undersampling\n",
    "\n",
    "Straightforward method that randomly samples our majority (non-fraudulent) cases to get a new set which is about equal to our minority (fraudulent) data. This can be convenient if there is a lot of data with a great deal of minority cases, however usually we do not want to throw away data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8667363",
   "metadata": {},
   "source": [
    "#### Random Oversampling\n",
    "Oversampling involves somehow creating more minority (fraudulent) data. The most straightforward method involves randomly duplicating the minority (fraudulent) datapoints. There is a function in `imblearn` which can do this. This trains the model on duplicates and isn't always ideal. You can tell the points are duplicated in the comparison plot because they are darker, having multiple points on the same location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed600567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "method = RandomOverSampler()\n",
    "X_resampled, y_resampled =  method.fit_resample(X, y)\n",
    "\n",
    "compare_plot(X, y, X_resampled, y_resampled, \n",
    "             method = \"RandomOverSampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2742dfb",
   "metadata": {},
   "source": [
    "#### Synthetic Minority Oversampling Technique (SMOTE)\n",
    "SMOTE is a technique which attempts to rectify imbalances by using the characteristics of neighbor minority data to create synthetic fraud cases. This avoids duplicating any data, is fairly realistic, but only works if the minority cases have similar features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prep_data function\n",
    "X, y = prep_data(df)\n",
    "\n",
    "# Define the resampling method\n",
    "method = SMOTE()\n",
    "\n",
    "# Create the resampled data set\n",
    "X_resampled, y_resampled = method.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of datapoints for non-fraudulent and fraudulent cases in the original data\n",
    "pd.value_counts(pd.Series(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee2e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of datapoints for non-fraudulent and fraudulent cases in the resampled data\n",
    "pd.value_counts(pd.Series(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6929fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot(X, y, X_resampled, y_resampled, method = 'SMOTE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd8233e",
   "metadata": {},
   "source": [
    "## Applying Fraud Detection Algorithms\n",
    "Generally there are two types of systems for detecting fraud: Rules Based and Machine Learning (ML) Based.  \n",
    "  \n",
    "Rules based uses a set of rules to catch fraud such as transactions occurring at odd zipcodes or too frequent transactions. This can catch fraud but also generates a lot of false positives. In addition they do not adapt over time, are limited to yes/no outcomes, and fail to recognize possible interactions between features.  \n",
    "  \n",
    "ML based adapt to data, using all the combined data to deliver a probability a transaction is fraudulent. This works much better and can be combined with a rules based approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209146a",
   "metadata": {},
   "source": [
    "### Traditional Fraud Detection\n",
    "Traditional fraud detection involves defining threshold values using common statistics for split fraud and non-fraud data, then use those thresholds to detect fraud. This is often done by looking at the means for differences.  \n",
    "  \n",
    "First we will clean up the data slightly and use Pandas `.groupby()` and `.mean()` functions to find the means of each column of interest split up by non-fraud and fraud cases. We can then look at this to create a rule which might catch fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d878ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "df.groupby('Class').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2149a",
   "metadata": {},
   "source": [
    "It looks like fraud cases have V1 < -3 and V3 < -5, so lets implement that as a rule. We will add a new column called `flag_as_fraud` and place a 1 where that rule is true and 0 where it is false using `numpy.where()`. We will compare this to if there is a case of actual fraud using `pandas.crossttab()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c10a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flag_as_fraud'] = np.where(np.logical_and(df.V1 < -3, df.V3 < -5), 1, 0)\n",
    "pd.crosstab(df.Class, df.flag_as_fraud,\n",
    "            rownames = ['Actual Fraud'], colnames = ['Flagged Fraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc507b7",
   "metadata": {},
   "source": [
    "With this rule 22/50 of fraud cases were detected, 28/50 were not detected, and there were 16 false positives. Not ideal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ff986",
   "metadata": {},
   "source": [
    "### Using ML Classification - Logistic Regression\n",
    "Lets use machine learning on our credit card data instead by implementing a Logistic Regression model.  \n",
    "  \n",
    "When fitting models there are a few things to keep in mind. The data should be first split into test and train data using functions such as `sklearn.model_selection.train_test_split()`. Then only the training data should be resampled. The model is then fitted to the resampled data. Then the results predicted from the model are to be compared using functions such as `sklearn.metrics.classification_report()` and `sklearn.metrics.confusion_matrix()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and testing sets, with 30% of the data used for our test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Fit a logistic regression model to our data\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Obtain model predictions\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Print the classifcation report and confusion matrix\n",
    "print('Classification report:\\n', classification_report(y_test, predicted))\n",
    "conf_mat = confusion_matrix(y_true = y_test, y_pred = predicted)\n",
    "print('Confusion matrix:\\n', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0a4e9",
   "metadata": {},
   "source": [
    "Here we get results that are significantly better than the rules based results. We find 8/10 fraud cases, miss 2/10, and have 1 false positive. The lower numbers here are because we only are using 30% of the data to test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563373d",
   "metadata": {},
   "source": [
    "### Combining Logistic Regression with SMOTE\n",
    "Here we will be trying to improve our results by resampling using SMOTE. We will be using the `pipeline` class from the `imblearn` package. This will allow us to combine both logistic regression and SMOTE as if they were a single machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which resampling method and which ML model to use in the pipeline\n",
    "# resampling = SMOTE(kind='borderline2')  # has been changed to BorderlineSMOTE\n",
    "resampling = BorderlineSMOTE()\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "# Combine the two into a single pipeline\n",
    "pipeline = Pipeline([('SMOTE', resampling), ('Logistic Regression', model)])\n",
    "\n",
    "# Create the training and testing sets, with 30% of the data used for our test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "#Fit the combined pipeline model to our data\n",
    "pipeline.fit(X_train, y_train) \n",
    "\n",
    "# Obtain model predictions\n",
    "predicted = pipeline.predict(X_test)\n",
    "\n",
    "# Obtain the results from the classification report and confusion matrix \n",
    "print('Classifcation report:\\n', classification_report(y_test, predicted))\n",
    "conf_mat = confusion_matrix(y_true = y_test, y_pred = predicted)\n",
    "print('Confusion matrix:\\n', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493192a3",
   "metadata": {},
   "source": [
    "Here there was a slight improvement over just using logistic regression. 10/10 fraud cases were caught, and 0/10 were missed. There were more false positives however. Resampling doesn't necessarily lead to better results. If fraud cases are scattered all over the data using SMOTE can introduce bias. The nearest neighbors of fraud cases aren't necessarily fraud cases themselves, and can confuse the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02595bfc",
   "metadata": {},
   "source": [
    "## Fraud detection using labeled data\n",
    "Now we will be learning how to flag fraudulent transaction with supervised learning, and comparing to find the most efficient fraud detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba444dc",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Classification is the problem of identifying which class a new observation belongs to based on a set of training data with known classes. Many classification problems relating to fraud are binary classification problems, meaning there are only two possible classes (yes/no, 1/0, True/False). In these cases we either want to assign new observations to:\n",
    "- 0: negative class ('majority' normal cases)\n",
    "- 1: positive class ('minority' fraud cases)\n",
    "  \n",
    "Here are four example classification methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ae55d",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Logistic Regression is one of the most used ML algorithms for binary classification, it can be adjusted well to work with imbalanced data which is common in fraud detection as we have shown earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03864d8d",
   "metadata": {},
   "source": [
    "#### Neural Network\n",
    "Neural Networks can also be used for fraud detection. They are capable of fitting highly non-linear models to data but are more complex to implement than other classifiers, thus we wont be going over them in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec8406",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "Decision Trees are commonly used for fraud detection. They provide very transparent results that are easily interpreted. They are however prone to overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d01951",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "Random Forests are similar to decision trees but are more robust. They involve the construction of multiple decision trees when training the model and output whatever class is either the mode or mean of the predicted classes from the individual trees. These trees are on a random subset of dataset features.  \n",
    "  \n",
    "Random Forests can handle complex data and are not prone to overfitting. They can be interpreted by looking at feature importance, and can be adjusted so that they work well with imbalanced data.  \n",
    "  \n",
    "The Drawback is that they are computationally complex.\n",
    "\n",
    "We will be optimizing a random forest model in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b254f20",
   "metadata": {},
   "source": [
    "##### Random Forest Implementation\n",
    "Once again we will be dealing with highly-imbalanced credit card data. First we create the DataFrame from our file and take a look at it so we know its similar to what we had in our last example. Then we use our `prep_data()` function defined earlier to create our X and y DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(cc2_file)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9376f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prep_data(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041eb9aa",
   "metadata": {},
   "source": [
    "The next step is to again use `test_train_split()` from `sklearn.model_selection` to create a set of training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fcd12a",
   "metadata": {},
   "source": [
    "And then we specify the `model` variable using `RandomForestClassifier()` then fit `model` to our data. For each tree the model will _bootstrap_  the training data, that is take a random sample with replacement. This randomness can be controlled by setting `random_state` to create consistent results. We can specify the number of trees created using \n",
    "`n_estimators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7608b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model as the random forest\n",
    "model = RandomForestClassifier(random_state = 5, n_estimators = 20)\n",
    "# Fit the model to our training set\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1a8c",
   "metadata": {},
   "source": [
    "The model has now been sucessfully applied to our data. Now to determine how well it worked and adjust to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9152a9",
   "metadata": {},
   "source": [
    "### Performance Evaluation\n",
    "As we have gone over in class, there are several performance metrics for fraud detection models. We have already learned that _Accuracy_ is a poor performance metric when working with highly imbalanced data. We have learned about the alternatives:\n",
    "- _Precision_: quantifies the correct positive predictions made\n",
    "- _Recall_: quantifies the number of correct positive predictions made out of all positive predictions that could have been made\n",
    "- _F-Measure_: combines both precision and recall into a single measure that captures both properties\n",
    "  \n",
    "We also have seen how to use the _Confusion Matrix_ to look at True/False Positives/Negatives.\n",
    "  \n",
    "In addition to those we can look at _Receiver Operating Characteristic (ROC) curves_ which plot the true positive rate vs the false positive rate at different threshold settings. This can be used to compare the performance of different algorithms. These are often summed up by computing the area under the ROC curve.\n",
    "[![Example of ROC curve](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/roc_curve.JPG)](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/roc_curve.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9cb49d",
   "metadata": {},
   "source": [
    "#### Getting basic performance metrics\n",
    "Now lets get the performance metrics from our RF model we just created. We can do this by using `.predict()` and `.predict_proba()` to obtain the predictions and probabilities from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324fb6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predictions from our random forest model \n",
    "predicted = model.predict(X_test)\n",
    "# Predict probabilities\n",
    "probs = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19dded",
   "metadata": {},
   "source": [
    "Next we can use `roc_auc_socre()` to get the ROC score, `classification_report()` to get the precision, recall, and f-score, and `confusion_matrix()` to get the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb36b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the ROC curve, classification report and confusion matrix\n",
    "print('ROC Score:')\n",
    "print(roc_auc_score(y_test, probs[:,1]))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, predicted))\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3385a0",
   "metadata": {},
   "source": [
    "Here we see we have a much higher precision (less false positives) but lower recall (more false negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e63f2",
   "metadata": {},
   "source": [
    "#### Plotting the Precision vs. Recall Curve\n",
    "\n",
    "The Precision-Recall curve allows us to investigate the trade-off between focusing on either of the two in our model. A good model balances the two. First we will need to calculate the average precision as well as the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average precision and the PR curve\n",
    "average_precision = average_precision_score(y_test, predicted)\n",
    "average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59907470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain precision and recall \n",
    "precision, recall, _ = precision_recall_curve(y_test, predicted)\n",
    "print(f'Precision: {precision}\\nRecall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50269ead",
   "metadata": {},
   "source": [
    "Now we define a function `plot_pr_curve()` which will plot our precision-recall curve for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4920916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(recall, precision, average_precision):\n",
    "    \"\"\"\n",
    "    https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "    \"\"\"\n",
    "    from inspect import signature\n",
    "    plt.figure()\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "\n",
    "    plt.step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n",
    "    plt.fill_between(recall, precision, alpha = 0.2, color = 'b', **step_kwargs)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(f'2-class Precision-Recall curve: AP={average_precision:0.2f}')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df597da",
   "metadata": {},
   "source": [
    "Running this code for our data gives the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the recall precision tradeoff\n",
    "plot_pr_curve(recall, precision, average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97efc540",
   "metadata": {},
   "source": [
    "### Adjusting algorithm weights\n",
    "When training a model we often want to tweak it to get the best recall-precision balance.  \n",
    "  \n",
    "`sklearn` has simple options to tweak its models for imbalanced data relating to the `class_weight` parameter which can be applied to Random Forests. \n",
    "- `class_weight = 'balanced'`: the model uses the values of y to automatically adjust weights inversely proportional to class frequencies in the the input data. This can also be applied to other classifiers like Logistic Regression and SVC\n",
    "- `class_weight = 'balanced_subsample'`: same as balanced, except weights are calculated again at each iteration of a growing tree. This only applies to Random Forests.\n",
    "- manual input: you can also manually adjust weights to any ratio, for example `class_weight={0:1,1:4}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ffd170",
   "metadata": {},
   "source": [
    "#### Balanced Subsample\n",
    "Lets try using the `balanced_subsample` option with our data. We will follow the same steps as earlier but now with `class_weight = 'balanced_subsample'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with balanced subsample\n",
    "model = RandomForestClassifier(class_weight = 'balanced_subsample', random_state = 5, n_estimators = 100)\n",
    "\n",
    "# Fit your training model to your training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Obtain the predicted values and probabilities from the model \n",
    "predicted = model.predict(X_test)\n",
    "probs = model.predict_proba(X_test)\n",
    "\n",
    "# Print the ROC curve, classification report and confusion matrix\n",
    "print('ROC Score:')\n",
    "print(roc_auc_score(y_test, probs[:,1]))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, predicted))\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5bdf0",
   "metadata": {},
   "source": [
    "Here there wasn't much improvement. The only major change is that false positives went down by 1. This was a nice simple option to try but we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e98013",
   "metadata": {},
   "source": [
    "#### Hyperparamaters\n",
    "Random Forest also has other options, one of which we mentioned previously. These options are called _hyperparamaters_ as they control the learning process of the algorithm. Here is an example of specifying a few potentially important ones.\n",
    "\n",
    "```{code-block}\n",
    "model = RandomForestClassifier(n_estimators = 10, \n",
    "                               criterion = ’gini’, \n",
    "                               max_depth = None, \n",
    "                               min_samples_split = 2, \n",
    "                               min_samples_leaf = 1, \n",
    "                               max_features = ’auto’\n",
    "                               n_jobs = -1\n",
    "                               class_weight = None)\n",
    "```\n",
    "\n",
    "- `n_estimators`: number of trees in the forest (very important)\n",
    "- `criterion`: changes the way the data is split at each node, defaults to the gini coefficient\n",
    "- `max_depth`, `min_samples_split`, `min_samples_leaf`, and `max_features`: some of the options determining the shape of the trees\n",
    "- `n_jobs`: specifies how many jobs to do in parallel (how many processors to use). This defaults to 1, and -1 uses all processors available.\n",
    "- `class_weight`: discussed previously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60775e",
   "metadata": {},
   "source": [
    "#### Adjusting RF manually\n",
    "It is possible to get better results by _assigning weights_ and _tweaking the shape_ of our decision trees.  \n",
    "  \n",
    "We will start by first defining a function `get_model_results()` which will fit a model on data and print the same preformance metrics as we have done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(X_train: np.ndarray, y_train: np.ndarray,\n",
    "                      X_test: np.ndarray, y_test: np.ndarray, model):\n",
    "    \"\"\"\n",
    "    model: sklearn model (e.g. RandomForestClassifier)\n",
    "    \"\"\"\n",
    "    # Fit your training model to your training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Obtain the predicted values and probabilities from the model \n",
    "    predicted = model.predict(X_test)\n",
    "    \n",
    "    try:\n",
    "        probs = model.predict_proba(X_test)\n",
    "        print('ROC Score:')\n",
    "        print(roc_auc_score(y_test, probs[:,1]))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    # Print the ROC curve, classification report and confusion matrix\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391be215",
   "metadata": {},
   "source": [
    "In our example we have 300 fraud to 7000 non-fraud cases, so if we set the weight ratio to 1:12 we would get 1/3 fraud to 2/3 non-fraud to train our model on each time it samples the training data.\n",
    "\n",
    "In addition we can set the criterion to entropy, maximum tree dept to 10, minimal samples in leaf nodes to 10, and the number of trees in the model to 20 to try and get a good result. These are just used as an example, we will go over the optimization of these options later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ceb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model options\n",
    "model = RandomForestClassifier(bootstrap = True,\n",
    "                               class_weight = {0:1, 1:12},\n",
    "                               criterion = 'entropy',\n",
    "                               # Change depth of model\n",
    "                               max_depth = 10,\n",
    "                               # Change the number of samples in leaf nodes\n",
    "                               min_samples_leaf = 10, \n",
    "                               # Change the number of trees to use\n",
    "                               n_estimators = 20,\n",
    "                               n_jobs = -1,\n",
    "                               random_state = 5)\n",
    "\n",
    "# Run the function get_model_results\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a45169",
   "metadata": {},
   "source": [
    "Here we can see the model has improved! The false negatives have gone down by 4, without compromising the false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e73e0",
   "metadata": {},
   "source": [
    "#### Parameter optimization with GridSearchCV\n",
    "`GridSearchCV` (from `sklearn.model_selection`) allows us to tweak our model parameters in a more systematic way. It evaluates all combinations of parameters as defined in a parameter grid (a dictionary relating parameters to their possible values) in relation to a scoring metric (precision, recall or f1) to determine which combination is best.  \n",
    "  \n",
    "We start by defining our parameter grid and define our model. Here we want to test:\n",
    "- 1 vs 30 for number of trees\n",
    "- gini vs entropy for criterion\n",
    "- auto vs log2 for max features\n",
    "- 4 vs 8 vs 10 vs 12 for max tree depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684871e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter sets to test\n",
    "param_grid = {'n_estimators': [1, 30],\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'max_features': ['auto', 'log2'], \n",
    "              'max_depth': [4, 8, 10, 12]}\n",
    "\n",
    "# Define the model to use\n",
    "model = RandomForestClassifier(random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcf215",
   "metadata": {},
   "source": [
    "Next we will call `GridSearchCV()` using our model and parameter grid to create a new model taking into account the parameter set. `cv = 5` specifies our cross-validation splitting strategy as being a 5-fold cross validation. `scoring = 'recall'` tells the function to optimize for recall.  `njobs = -1` will allow GridSearchCV to use all of our processor cores to speed up the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd0a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the parameter sets with the defined model\n",
    "CV_model = GridSearchCV(estimator = model, param_grid = param_grid, cv = 5, scoring = 'recall', n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843adc13",
   "metadata": {},
   "source": [
    "Finally we fit the model the same as done previously. We can use `.best_params_` to get the best parameter from our fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to our training data and obtain best parameters\n",
    "CV_model.fit(X_train, y_train)\n",
    "CV_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18e1bf",
   "metadata": {},
   "source": [
    "So it appears that from our options the best criterion is `'gini'`, the max depth should be 8, the max features should be `'log2'` and the number of trees should be 30. Now lets apply this information by adjusting the parameter as we have done before and generating a report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4492015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the optimal parameters in the model\n",
    "model = RandomForestClassifier(class_weight = {0:1,1:12},\n",
    "                               criterion = 'gini',\n",
    "                               max_depth = 8,\n",
    "                               max_features = 'log2', \n",
    "                               min_samples_leaf = 10,\n",
    "                               n_estimators = 30,\n",
    "                               n_jobs = -1,\n",
    "                               random_state = 5)\n",
    "\n",
    "# Get results from your model\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebbd9b",
   "metadata": {},
   "source": [
    "When compared to the balanced subsample results false negatives went down by 3, however false positives also went up by three. To determine which model is best decisions should be made based on how important it is to catch fraud vs how many false positives can be dealt with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b3091",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "Ensemble methods create multiple machine learning models and combine them to produce a final result which is usually more accurate than a single model alone. These methods take into account a selection of models and average them to produce a final model. This ensures predictions are robust, there is less overfitting, and improves prediction performance (particularly in the case of models with different recall and precision scores). Many Kaggle competition winners use ensemble models.  \n",
    "  \n",
    "Random Forests are an example of an ensemble method, as it is an ensemble of decision trees. It uses the _bootstrap aggregation_ or _bagging ensemble_method for creating an ensemble method. Here models are trained on random subsamples of data and the results from each model are aggregated by taking the average prediction of all the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc9803",
   "metadata": {},
   "source": [
    "#### Stacking Ensemble Methods\n",
    "\n",
    "One way of creating an ensemble method is by stacking. In this multiple models are trained on the entire training dataset. These models are then combined via a \"voting\" method where the classification probabilities from each model are compared. This is often done with models who differ from one another, unlike bagging ensembles which often use the same type of model multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfcc0d6",
   "metadata": {},
   "source": [
    "Lets try to improve upon a logistic regression model by combining it with a random forest and decision tree. First we need to establish the baseline of the logistic regression model alone the same way we have done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Logistic Regression model with weights\n",
    "model = LogisticRegression(class_weight = {0:1, 1:15}, random_state = 5, solver = 'liblinear')\n",
    "\n",
    "# Get the model results\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91316d3",
   "metadata": {},
   "source": [
    "Here we see that the logistic regression had a great deal more false positives than the random forest, but also had a better recall. This means that combing them in an ensemble method would be useful. Lets start by defining the three models we will use in our ensemble:\n",
    "- Logistic Regression from before\n",
    "- Random Forest from before\n",
    "- Decision Tree with balanced class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26719990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the three classifiers to use in the ensemble\n",
    "clf1 = LogisticRegression(class_weight = {0:1, 1:15},\n",
    "                          random_state = 5,\n",
    "                          solver = 'liblinear')\n",
    "\n",
    "clf2 = RandomForestClassifier(class_weight = {0:1, 1:12}, \n",
    "                              criterion = 'gini', \n",
    "                              max_depth = 8, \n",
    "                              max_features = 'log2',\n",
    "                              min_samples_leaf = 10, \n",
    "                              n_estimators = 30, \n",
    "                              n_jobs = -1,\n",
    "                              random_state = 5)\n",
    "\n",
    "clf3 = DecisionTreeClassifier(random_state = 5,\n",
    "                              class_weight = \"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90640c36",
   "metadata": {},
   "source": [
    "Now we can use `VotingClassifier()` from `sklearn.ensemble` to combine the three. We can specify the `voting` parameter to be `'hard'` or `'soft'`. `'hard'` will use the predicted class labels and take the majority vote (i.e. 2/3 of the models said this case is fraud, therefore it is fraud). `'soft'` will take the average probability of a class by combining the individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ba580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the classifiers in the ensemble model\n",
    "ensemble_model = VotingClassifier(estimators = [('lr', clf1), ('rf', clf2), ('dt', clf3)],\n",
    "                                  voting = 'hard')\n",
    "\n",
    "# Get the results \n",
    "get_model_results(X_train, y_train, X_test, y_test, ensemble_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e4db28",
   "metadata": {},
   "source": [
    "Here we can see that the number of false positives has dramatically reduced, while we are getting the smallest amount of false negatives of any algorithm we have tried so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23838d4",
   "metadata": {},
   "source": [
    "##### Adjusting weights within the Voting Classifier\n",
    "\n",
    "We can potentially improve performance even more by adjusting the weights given to each model in our ensemble. This allows us to change how much emphasis we place on a particular model relative to the others. This can be done by specifying a list for the `weights` parameter in `VotingClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ae834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ensemble model\n",
    "ensemble_model = VotingClassifier(estimators = [('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "                                  voting = 'soft', weights = [1, 4, 1],\n",
    "                                  flatten_transform = True)\n",
    "\n",
    "# Get results \n",
    "get_model_results(X_train, y_train, X_test, y_test, ensemble_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da96f19",
   "metadata": {},
   "source": [
    "Playing around with weights will allow us to tweak the performance of our model even further to achieve the kind of results we are looking for. In this case it decreased the false positives by 4, but increased the false positives by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6380413",
   "metadata": {},
   "source": [
    "## Fraud Detection using unlabled data\n",
    "\n",
    "Oftentimes the data we get isn't prelabled as fraudulent. In that case we need to use unsupervised learning techniques to detect fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8f98e",
   "metadata": {},
   "source": [
    "### Normal vs Abnormal Behavior\n",
    "\n",
    "To do this we must make a distinction between normal and abnormal behavior. Abnormal behavior isn't necessarily fraudulent, but it can be used to make a determination on how likely a case is fraud. This is generally difficult since it is difficult to validate your data.  \n",
    "  \n",
    "When looking for abnormal behavior there are a few things to consider:\n",
    "- thoroughly describe the data:\n",
    "    - plot histograms\n",
    "    - check for outliers\n",
    "    - investigate correlations\n",
    "- Are there any known historic cases of fraud? What typifies those cases?\n",
    "- Investigate whether the data is homogeneous, or whether different types of clients display different behavior\n",
    "- Check patterns within subgroups of data: is your data homogeneous?\n",
    "- Verify data points are the same type:\n",
    "    - individuals\n",
    "    - groups\n",
    "    - companies\n",
    "    - governmental organizations\n",
    "- Do the data points differ on:\n",
    "    - spending patterns\n",
    "    - age\n",
    "    - location\n",
    "    - frequency\n",
    "- For credit card fraud, location can be an indication of fraud\n",
    "- This goes for e-commerce sites\n",
    "    - where's the IP address located and where is the product ordered to ship?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602159cd",
   "metadata": {},
   "source": [
    "#### Exploring the Data\n",
    "\n",
    "Here we will be looking at payment transaction data. Transactions are categorized by type of expense and amount spent. We also have another data file with some information on client characteristics such as age group and gender. Some transactions have been labeled as fraud which we will use to validate our results later. To understand what is normal you need a good understanding of the data and its characteristics.  \n",
    "  \n",
    "Lets get our data into some DataFrames and get an idea of what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3659224",
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_df = pd.read_csv(banksim_file)\n",
    "banksim_df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "banksim_adj_df = pd.read_csv(banksim_adj_file)\n",
    "banksim_adj_df.drop(['Unnamed: 0'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_df.shape # 7200 rows and 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc110a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_adj_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_adj_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c6c65",
   "metadata": {},
   "source": [
    "Now we will use `groupby` from `pandas` to take a mean of the data based on transaction type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bfa9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_df.groupby(['category']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3023aed",
   "metadata": {},
   "source": [
    "Here we can already see that the majority of fraud is in travel, leisure, and sports related transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6893a2",
   "metadata": {},
   "source": [
    "#### Customer Segmentation\n",
    "\n",
    "Lets look for some obvious patterns in the data to determine if we need to segment the data into groups or if it's fairly homogeneous. Lets look at age groups and see if there is any significant difference in behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_df.groupby(['age']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_df.age.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e8db48",
   "metadata": {},
   "source": [
    "Since the largest age groups are relatively similar, we should probably not split the data into age segments before running fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95e822",
   "metadata": {},
   "source": [
    "#### Using statistics to define normal behavior\n",
    "\n",
    "Lets see how fraudulent transactions differ structurally from normal transactions by looking at the average amounts spent. We will create two new dataframes for fraud and non fraud data then plot the two as histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ac5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two dataframes with fraud and non-fraud data \n",
    "df_fraud = banksim_df[banksim_df.fraud == 1] \n",
    "df_non_fraud = banksim_df[banksim_df.fraud == 0]\n",
    "\n",
    "# Plot histograms of the amounts in fraud and non-fraud data \n",
    "plt.hist(df_fraud.amount, alpha = 0.5, label = 'fraud')\n",
    "plt.hist(df_non_fraud.amount, alpha = 0.5, label = 'nonfraud')\n",
    "plt.xlabel('amount')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d08bac",
   "metadata": {},
   "source": [
    "Since there are less fraudulent transactions lets take a look at only a histogram of them to see their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_fraud.amount, alpha = 0.5, label = 'fraud')\n",
    "plt.xlabel('amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb3818",
   "metadata": {},
   "source": [
    "Here we see that fraudulent transactions tend to be on the larger side! This will help in distinguishing fraud from non-fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa51ab",
   "metadata": {},
   "source": [
    "### Clustering methods to detect fraud\n",
    "\n",
    "\n",
    "The objective of any clustering model is to detect patterns in data. Specifically it is to group the data into distinct data clusters made of points similar to each other but distinct from other points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc7150",
   "metadata": {},
   "source": [
    "#### Scaling the data\n",
    "For any ML algorithm using distance its crucial to always scale your data, so lets do that first. We can use `MinMaxScaler` from `sklearn.preprocessing` to scale our data.  \n",
    "  \n",
    "First we will create a Numpy array from our dataframe only containing the values from df as floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34dd9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = banksim_adj_df.fraud\n",
    "\n",
    "cols = ['age', 'amount', 'M', 'es_barsandrestaurants', 'es_contents',\n",
    "        'es_fashion', 'es_food', 'es_health', 'es_home', 'es_hotelservices',\n",
    "        'es_hyper', 'es_leisure', 'es_otherservices', 'es_sportsandtoys',\n",
    "        'es_tech', 'es_transportation', 'es_travel']\n",
    "\n",
    "# Take the float values of df for X\n",
    "X = banksim_adj_df[cols].values.astype(np.float)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb2a2bc",
   "metadata": {},
   "source": [
    "Then we can define the scaler and apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler and apply to the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17a01d",
   "metadata": {},
   "source": [
    "#### K-means clustering\n",
    "K-means clustering tries to minimize the sum of all distances between the data samples and their associated cluster centroids. The score of K-means clustering is the inverse of that minimization, so should be close to 0 ideally. It is a straightforward and relatively powerful method of predicting suspicious cases. With very large data however MiniBatch K-means is a more efficient way to implement K-means and is what we will be doing.  \n",
    "  \n",
    "We can use `MiniBatchKMeans` from `sklearn` to do so. We will specify that there is to be 8 clusters and set the random state to 0 to have repeatable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model \n",
    "kmeans = MiniBatchKMeans(n_clusters = 8, random_state = 0)\n",
    "\n",
    "# Fit the model to the scaled data\n",
    "kmeans.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572b781",
   "metadata": {},
   "source": [
    "#### Elbow Method for determining clusters\n",
    "While we picked 8 clusters in the previous example, this might not be ideal! It's very important to get the number of clusters correct, particularly when doing fraud detection. There are a few ways to do so, but here we apply the _Elbow Method_ to do just that. To do this we will generate an elbow curve which scores each model and plots them vs the number of clusters in them.  \n",
    "  \n",
    "In this example we will be looking at 1 to 10 clusters, running MiniBatch K-means on all the clusters in this range, fit the models, and plotting them with their respective scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0951165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of clusters to try\n",
    "clustno = range(1, 10)\n",
    "\n",
    "# Run MiniBatch Kmeans over the number of clusters\n",
    "kmeans = [MiniBatchKMeans(n_clusters = i) for i in clustno]\n",
    "\n",
    "# Obtain the score for each model\n",
    "score = [kmeans[i].fit(X_scaled).score(X_scaled) for i in range(len(kmeans))]\n",
    "\n",
    "# Plot the models and their respective score \n",
    "plt.plot(clustno, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33369b",
   "metadata": {},
   "source": [
    "To determine which is the ideal number of clusters we look for the _elbow_ of the curve. That is where the score begins to increase less as the number of clusters increase. In this case it is at 3 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0d227",
   "metadata": {},
   "source": [
    "### Assigning fraud vs non-fraud\n",
    "The general method of assigning fraud after clustering is to take the outliers of each cluster and flag those as fraud. We do this by finding the distance of each point to their cluster's centroid and establishing a cutoff distance beyond which a point is an outlier (i.e. 95%). These outliers are abnormal or suspicious, but aren't necessarily fraudulent.  \n",
    "[![Visualization of finding outliers in clustered data](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/clusters_4.JPG)](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/clusters_4.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb04920",
   "metadata": {},
   "source": [
    "#### Detecting Outliers\n",
    "To do this first we will split our data into training and testing sets as we have done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f67209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, labels, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594ff95",
   "metadata": {},
   "source": [
    "Then we will define our K-means model with 3 clusters as we determined with the elbow test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-means model \n",
    "kmeans = MiniBatchKMeans(n_clusters = 3, random_state = 42).fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ee612",
   "metadata": {},
   "source": [
    "Now we need to get the cluster predictions as well as the cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predictions and calculate distance from cluster centroid\n",
    "X_test_clusters = kmeans.predict(X_test)\n",
    "X_test_clusters_centers = kmeans.cluster_centers_\n",
    "dist = [np.linalg.norm(x-y) for x, y in zip(X_test, X_test_clusters_centers[X_test_clusters])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8217578d",
   "metadata": {},
   "source": [
    "Finally we define the boundry between fraud and non-fraud at 95% of the distance distribution or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11864948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fraud predictions based on outliers on clusters \n",
    "km_y_pred = np.array(dist)\n",
    "km_y_pred[dist >= np.percentile(dist, 95)] = 1\n",
    "km_y_pred[dist < np.percentile(dist, 95)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd2de1",
   "metadata": {},
   "source": [
    "#### Checking model results\n",
    "Lets create some preformance metrics to see how well this was at detecting fraud.  \n",
    "  \n",
    "Lets define a function that can create a nice looking confusion matrix first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes = ['Not Fraud', 'Fraud'],\n",
    "                          normalize = False,\n",
    "                          title = 'Fraud Confusion matrix',\n",
    "                          cmap = plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    From:\n",
    "        http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-\n",
    "        examples-model-selection-plot-confusion-matrix-py\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    # print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment = \"center\",\n",
    "                 color = \"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e7b5d",
   "metadata": {},
   "source": [
    "Now lets get the ROC score and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC Score:')\n",
    "print(roc_auc_score(y_test, km_y_pred))\n",
    "print()\n",
    "\n",
    "# Create a confusion matrix\n",
    "km_cm = confusion_matrix(y_test, km_y_pred)\n",
    "\n",
    "# Plot the confusion matrix in a figure to visualize results \n",
    "plot_confusion_matrix(km_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85831485",
   "metadata": {},
   "source": [
    "This wasn't nearly as good as our models created with labled data, however it does work for detecting fraud!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22fa68a",
   "metadata": {},
   "source": [
    "### Other methods: DBSCAN\n",
    "K-means works well with data clustered into normal, round shapes but has it's downsides. It can't handle any data clusters not in that shape. It can also end up identifying multiple outliers as their own smaller cluster. There are many other clustering methods out there.\n",
    "\n",
    "[![Examples of different clustering methods](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/clustering_methods.JPG)](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/clustering_methods.JPG)\n",
    "  \n",
    "One that we will look at today is DBSCAN. DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. It does not require the number of clusters to be predefined. Instead it finds core samples of high density and expands clusters from them. This means it works well with data containing clusters of similar density. It can be used to ID fraud as very small clusters. It does require you to assign the maximum allowed distance between points in a cluster and the minimum points which constitute a cluster. It has the best performance on weirdly shaped data, however it is computationally heavier than MiniBatch K-means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff545bc8",
   "metadata": {},
   "source": [
    "#### Implementing DBSCAN\n",
    "`DBSCAN` is available from `sklearn.cluster`. We will need to set the max distance between samples (0.9) and minimum observations (10) to fit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a07555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the DBscan model\n",
    "db = DBSCAN(eps = 0.9, min_samples = 10, n_jobs = -1).fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b7108",
   "metadata": {},
   "source": [
    "Now lets see how many clusters we have and the preformance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8088871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predicted labels and calculate number of clusters\n",
    "pred_labels = db.labels_\n",
    "n_clusters = len(set(pred_labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "# Print performance metrics for DBscan\n",
    "print(f'Estimated number of clusters: {n_clusters}')\n",
    "print(f'Homogeneity: {homogeneity_score(labels, pred_labels):0.3f}')\n",
    "print(f'Silhouette Coefficient: {silhouette_score(X_scaled, pred_labels):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37c1021",
   "metadata": {},
   "source": [
    "#### Assessing smallest clusters\n",
    "We now need to filter out the smallest clusters from the 23 we identified. We will start by counting the samples in each cluster by running a bincount on the cluster numbers under `pred_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da4db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count observations in each cluster number\n",
    "counts = np.bincount(pred_labels[pred_labels >= 0])\n",
    "\n",
    "# Print the result\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418287b3",
   "metadata": {},
   "source": [
    "We will then sort `counts` and take the three smallest clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ef3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the sample counts of the clusters and take the top 3 smallest clusters\n",
    "smallest_clusters = np.argsort(counts)[:3]\n",
    "\n",
    "# Print the results \n",
    "print(f'The smallest clusters are clusters: {smallest_clusters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90019a57",
   "metadata": {},
   "source": [
    "Within `counts`, we will select only these smallest clusters and print the number of samples in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee54dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the counts of the smallest clusters only\n",
    "print(f'Their counts are: {counts[smallest_clusters]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0c156",
   "metadata": {},
   "source": [
    "#### Verifying our Results\n",
    "While in reality you usually don't have labels to do this, we can verify our results to see how well DBSCAN did.  \n",
    "  \n",
    "First we will create a dataframe combining the cluster numbers and their actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef164fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the predicted cluster numbers and fraud labels \n",
    "df = pd.DataFrame({'clusternr':pred_labels,'fraud':labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65b8a2",
   "metadata": {},
   "source": [
    "Next we will create a condition that flags fraud for the three smallest clusters: 21, 17, and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a condition flagging fraud for the smallest clusters \n",
    "df['predicted_fraud'] = np.where((df['clusternr'].isin([21, 17, 9])), 1 , 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f817a8",
   "metadata": {},
   "source": [
    "Finally we run a crosstab on our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef4f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a crosstab on the results \n",
    "print(pd.crosstab(df['fraud'], df['predicted_fraud'],\n",
    "                  rownames = ['Actual Fraud'],\n",
    "                  colnames = ['Flagged Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a83f3",
   "metadata": {},
   "source": [
    "For our flagged cases roughly 2/3 are actually fraud! Because we only took the three smallest clusters we flag less cases of fraud, causing more false negatives. We could increase this number, however this will risk increasing the number of false positives in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33956c90",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "Now that we are done using our data files we are going to get rid of them. I hope this was informative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d3adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(\"data/chapter_1\"):\n",
    "#     shutil.rmtree(\"data/chapter_1\")\n",
    "\n",
    "# if os.path.exists(\"data/chapter_2\"):\n",
    "#     shutil.rmtree(\"data/chapter_2\")\n",
    "\n",
    "# if os.path.exists(\"data/chapter_3\"):\n",
    "#     shutil.rmtree(\"data/chapter_3\")\n",
    "        \n",
    "# if os.path.exists(\"data/chapter_4\"):\n",
    "#     shutil.rmtree(\"data/chapter_4\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   13,
   17,
   21,
   25,
   29,
   33,
   37,
   41,
   45,
   49,
   56,
   60,
   64,
   102,
   106,
   111,
   115,
   119,
   158,
   184,
   191,
   197,
   203,
   209,
   213,
   217,
   221,
   244,
   250,
   255,
   276,
   282,
   287,
   295,
   300,
   311,
   316,
   321,
   323,
   332,
   339,
   342,
   346,
   350,
   354,
   361,
   376,
   380,
   385,
   407,
   411,
   416,
   426,
   431,
   436,
   441,
   452,
   457,
   462,
   466,
   468,
   472,
   475,
   480,
   485,
   489,
   502,
   507,
   512,
   516,
   524,
   528,
   534,
   540,
   544,
   548,
   568,
   572,
   575,
   585,
   590,
   608,
   612,
   634,
   641,
   665,
   671,
   687,
   691,
   702,
   711,
   715,
   718,
   722,
   726,
   730,
   743,
   747,
   754,
   760,
   764,
   770,
   777,
   794,
   798,
   805,
   809,
   815,
   823,
   827,
   833,
   861,
   869,
   876,
   880,
   884,
   888,
   890,
   894,
   896,
   900,
   906,
   910,
   912,
   916,
   922,
   933,
   937,
   941,
   945,
   952,
   959,
   971,
   975,
   979,
   986,
   992,
   999,
   1015,
   1019,
   1025,
   1030,
   1033,
   1037,
   1040,
   1044,
   1049,
   1053,
   1058,
   1065,
   1103,
   1107,
   1117,
   1121,
   1130,
   1135,
   1138,
   1142,
   1151,
   1156,
   1162,
   1166,
   1172,
   1176,
   1179,
   1186,
   1189,
   1193,
   1196,
   1200,
   1205,
   1209,
   1214
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}